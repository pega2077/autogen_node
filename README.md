# autogen_node

A Node.js/TypeScript implementation of [microsoft/autogen](https://github.com/microsoft/autogen), providing a framework for building multi-agent AI systems with conversational agents.

## Overview

This project brings the powerful multi-agent orchestration capabilities of Microsoft's AutoGen framework to the Node.js ecosystem. It's designed based on the .NET code structure and class definitions, providing a familiar API for developers working with AutoGen in different languages.

## Features

- **Event-Driven Architecture (AutoGen v0.4)**: Asynchronous message passing and distributed agent systems
  - **AgentRuntime**: Core runtime for hosting and managing agents
  - **Direct Messaging**: Send messages between agents asynchronously
  - **Publish/Subscribe**: Topic-based broadcast messaging
  - **Cancellation Tokens**: Control and cancel async operations
  - **State Management**: Persist and restore runtime state
- **Base Agent Framework**: Core interfaces and abstract classes for building custom agents
- **Multiple LLM Providers**: Support for OpenAI, OpenRouter, Ollama, Anthropic, and Google Gemini
  - **OpenAI**: GPT-3.5, GPT-4, and other OpenAI models
  - **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus, and Claude 3 Haiku
  - **Google Gemini**: Gemini 1.5 Flash, Gemini 1.5 Pro, and Gemini Pro
  - **OpenRouter**: Access to 100+ models from multiple providers
  - **Ollama**: Run LLMs locally for privacy and offline use
- **AssistantAgent**: LLM-powered conversational agent with provider flexibility
- **UserProxyAgent**: Human-in-the-loop agent for interactive conversations
- **Group Chat**: Multi-agent collaboration system for complex tasks
- **Function Calling**: Register and execute custom functions with agents
- **Code Execution**: Automatically execute code generated by agents (JavaScript, Python, Bash)
- **Memory System**: Persistent memory for agents to maintain context across conversations (based on Microsoft AutoGen)
- **Type-Safe**: Built with TypeScript for enhanced developer experience
- **Flexible Message System**: Support for different message types and roles
- **Conversation Management**: Built-in conversation history and state management

## Installation

```bash
npm install
```

## Quick Start

### Using OpenAI (Default)

```typescript
import { AssistantAgent, UserProxyAgent, HumanInputMode } from './src/index';

// Create an AI assistant
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openai',  // optional, this is the default
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful assistant.',
  model: 'gpt-3.5-turbo',
  temperature: 0
});

// Create a user proxy for human interaction
const userProxy = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.ALWAYS
});

// Start a conversation
await userProxy.initiateChat(
  assistant,
  'Hello! Can you help me?',
  10 // max rounds
);
```

### Using OpenRouter

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openrouter',
  apiKey: process.env.OPENROUTER_API_KEY!,
  model: 'anthropic/claude-2',
  temperature: 0.7
});
```

### Using Anthropic Claude

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'anthropic',
  apiKey: process.env.ANTHROPIC_API_KEY!,
  model: 'claude-3-5-sonnet-20241022',
  temperature: 0.7
});
```

### Using Google Gemini

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'gemini',
  apiKey: process.env.GEMINI_API_KEY!,
  model: 'gemini-1.5-flash',
  temperature: 0.7
});
```

### Using Ollama (Local)

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'ollama',
  model: 'llama2',
  temperature: 0.7
});
```

See [LLM_PROVIDERS.md](./LLM_PROVIDERS.md) for detailed provider documentation.


## Project Structure

```
autogen_node/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/                 # Core interfaces and base classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ IAgent.ts         # Agent interface definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BaseAgent.ts      # Base agent implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ IFunctionCall.ts  # Function calling interfaces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FunctionContract.ts # Function contract builder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FunctionCallMiddleware.ts # Function execution middleware
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ICodeExecutor.ts  # Code execution interface
‚îÇ   ‚îú‚îÄ‚îÄ agents/               # Agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AssistantAgent.ts # LLM-powered assistant with function calling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ UserProxyAgent.ts # Human proxy with code execution
‚îÇ   ‚îú‚îÄ‚îÄ executors/            # Code execution implementations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LocalCodeExecutor.ts # Local code executor
‚îÇ   ‚îú‚îÄ‚îÄ providers/            # LLM provider implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OpenAIProvider.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OpenRouterProvider.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ OllamaProvider.ts
‚îÇ   ‚îú‚îÄ‚îÄ examples/             # Example applications
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basic-chat.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ function-calling-example.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ code-execution-example.ts
‚îÇ   ‚îî‚îÄ‚îÄ index.ts              # Main export file
‚îú‚îÄ‚îÄ dist/                     # Compiled JavaScript output
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îî‚îÄ‚îÄ README.md
```

## Architecture

This implementation follows the AutoGen architecture with both traditional and event-driven patterns:

### Event-Driven Architecture (AutoGen v0.4)

The new event-driven architecture enables scalable, distributed multi-agent systems:

1. **AgentRuntime**: Core runtime for hosting and managing agents
   - `sendMessage()`: Direct asynchronous message passing
   - `publishMessage()`: Topic-based broadcast messaging
   - Agent registration and lifecycle management
   - State persistence and restoration

2. **AgentId & TopicId**: Distributed agent addressing
   - Unique identification for agents across processes
   - Topic-based message routing

3. **CancellationToken**: Async operation control
   - Cancel long-running operations
   - Cleanup on cancellation

See [EVENT_DRIVEN.md](./EVENT_DRIVEN.md) for detailed documentation.

### Traditional Architecture

1. **IAgent Interface**: Defines the contract for all agents
   - `generateReply()`: Generate responses to messages
   - `getName()`: Get the agent's name

2. **BaseAgent**: Abstract base class providing:
   - Conversation history management
   - Message sending and receiving
   - Chat initiation logic
   - Termination detection

3. **Agent Implementations**:
   - **AssistantAgent**: Uses LLM providers for intelligent responses with function calling support
   - **UserProxyAgent**: Facilitates human interaction with configurable input modes and code execution

4. **Function Calling**: Enable agents to call custom functions
   - Define functions with `FunctionContract`
   - Automatic function execution via `FunctionCallMiddleware`
   - OpenAI-compatible function definitions

5. **Code Execution**: Execute code generated by agents
   - `LocalCodeExecutor` for JavaScript, Python, and Bash
   - Automatic code extraction from markdown code blocks
   - Safe execution in temporary directories
### Message System

Messages follow a structured format:
```typescript
interface IMessage {
  content: string;
  role: 'user' | 'assistant' | 'system' | 'function' | 'tool';
  name?: string;
  functionCall?: {
    name: string;
    arguments: string;
  };
  toolCalls?: Array<{
    id: string;
    type: 'function';
    function: {
      name: string;
      arguments: string;
    };
  }>;
  toolCallId?: string;
}
```

## Configuration

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here  # Optional
GEMINI_API_KEY=your_gemini_api_key_here        # Optional
OPENROUTER_API_KEY=your_openrouter_api_key_here  # Optional
# OLLAMA_BASE_URL=http://localhost:11434/v1      # Optional
```

## Scripts

```bash
# Build the project
npm run build

# Run the basic interactive example (OpenAI)
npm run example:basic

# Run the automated two-agent conversation (OpenAI)
npm run example:auto

# Run the group chat example (OpenAI)
npm run example:group

# Run Anthropic Claude example
npm run example:anthropic

# Run Google Gemini example
npm run example:gemini

# Run OpenRouter example
npm run example:openrouter

# Run Ollama example (local LLM)
npm run example:ollama

# Run function calling example
npm run example:functions

# Run code execution example
npm run example:code

# Run memory example
npm run example:memory

# Run event-driven architecture example (AutoGen v0.4)
npm run example:events

# Run tests
npm test

# Run tests with coverage
npm run test:coverage

# Development mode with auto-reload
npm run dev

# Clean build artifacts
npm run clean
```

## Examples

### Event-Driven Architecture (AutoGen v0.4)

```typescript
import {
  AgentId,
  TopicId,
  SingleThreadedAgentRuntime,
  createSubscription,
} from './src/index';

// Create event-driven agent
class EventAgent {
  async handleMessage(message: any, sender: AgentId | null) {
    return {
      role: 'assistant',
      content: `Processed: ${message.content}`,
    };
  }
}

// Create runtime and register agents
const runtime = new SingleThreadedAgentRuntime();
const agent = new EventAgent();
const agentId = new AgentId('event_agent', 'agent1');

await runtime.registerAgentInstance(agent, agentId);

// Direct message passing
const response = await runtime.sendMessage(
  { content: 'Hello!' },
  agentId
);

// Topic-based pub/sub
const topic = new TopicId('notifications', 'system');
await runtime.addSubscription(
  createSubscription('sub1', topic, agentId)
);
await runtime.publishMessage(
  { content: 'Broadcast message' },
  topic
);
```

See [EVENT_DRIVEN.md](./EVENT_DRIVEN.md) for complete documentation.

### Basic Two-Agent Chat

```typescript
import { AssistantAgent, UserProxyAgent, HumanInputMode } from './src/index';

const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful math tutor.',
  model: 'gpt-3.5-turbo'
});

const user = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.ALWAYS
});

await user.initiateChat(assistant, 'Help me solve 2x + 3 = 7', 10);
```

### Automated Conversation (No Human Input)

```typescript
const user = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.NEVER
});

// Agent will auto-reply without human intervention
```

### Function Calling

```typescript
import { AssistantAgent, FunctionContract } from './src/index';

// Define a weather function
const getWeather = FunctionContract.fromFunction(
  'get_weather',
  'Get the current weather for a location',
  [
    {
      name: 'location',
      type: 'string',
      description: 'The city and state, e.g. San Francisco, CA',
      required: true
    }
  ],
  async (location: string) => {
    // Your weather API logic here
    return `The weather in ${location} is sunny, 72¬∞F`;
  }
);

// Create assistant with functions
const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful assistant with access to weather data.',
  model: 'gpt-3.5-turbo',
  functions: [getWeather]
});

// The assistant will automatically call the function when needed
await userProxy.initiateChat(assistant, "What's the weather in San Francisco?", 3);
```

### Code Execution

```typescript
import { AssistantAgent, UserProxyAgent, LocalCodeExecutor, HumanInputMode } from './src/index';

// Create code executor
const codeExecutor = new LocalCodeExecutor();

// Create assistant that writes code
const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a coding assistant. Write code in markdown code blocks.',
  model: 'gpt-3.5-turbo'
});

// Create user proxy with code execution enabled
const userProxy = new UserProxyAgent({
  name: 'user_proxy',
  humanInputMode: HumanInputMode.NEVER,
  codeExecutor: codeExecutor,
  autoExecuteCode: true
});

// The agent will write code, and it will be automatically executed
await userProxy.initiateChat(
  assistant,
  'Write JavaScript code to calculate the sum of numbers from 1 to 100',
  3
);

await codeExecutor.cleanup();
```

### Group Chat with Multiple Agents

```typescript
import { AssistantAgent, GroupChat, GroupChatManager } from './src/index';

// Create multiple specialized agents
const designer = new AssistantAgent({
  name: 'designer',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a creative designer.',
  model: 'gpt-3.5-turbo'
});

const engineer = new AssistantAgent({
  name: 'engineer',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a practical engineer.',
  model: 'gpt-3.5-turbo'
});

// Create group chat
const groupChat = new GroupChat({
  agents: [designer, engineer],
  maxRound: 10
});

// Create manager
const manager = new GroupChatManager({
  groupChat: groupChat
});

// Run the discussion
await manager.runChat('Design a new mobile app feature');
```

### Memory Usage

Memory allows agents to maintain context across conversations:

```typescript
import { AssistantAgent, ListMemory, MemoryMimeType } from './src/index';

// Create memory instance
const userMemory = new ListMemory({ name: 'user_preferences' });

// Add memory content
await userMemory.add({
  content: 'User prefers formal language',
  mimeType: MemoryMimeType.TEXT,
  metadata: { timestamp: Date.now() }
});

await userMemory.add({
  content: 'User is interested in TypeScript and AI',
  mimeType: MemoryMimeType.TEXT
});

// Create agent with memory
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openai',
  apiKey: process.env.OPENAI_API_KEY!,
  memory: [userMemory]
});

// Memory is automatically injected into context
const reply = await assistant.generateReply([
  { role: 'user', content: 'What should I learn next?' }
]);
```

For more details, see [MEMORY.md](MEMORY.md).

## Comparison with microsoft/autogen

This project implements core features from microsoft/autogen for Node.js/TypeScript.

### ‚úÖ Implemented Features

| Feature | Status |
|---------|--------|
| Base Agent Framework | ‚úÖ Complete |
| AssistantAgent | ‚úÖ Complete |
| UserProxyAgent | ‚úÖ Complete |
| Group Chat | ‚úÖ Complete |
| Multiple LLM Providers | ‚úÖ OpenAI, Anthropic, Gemini, OpenRouter, Ollama |
| Function Calling | ‚úÖ Complete |
| Code Execution | ‚úÖ JavaScript, Python, Bash |
| Memory System | ‚úÖ ListMemory |
| Event-Driven Architecture (v0.4) | ‚úÖ Complete |
| AgentRuntime | ‚úÖ SingleThreadedAgentRuntime |
| Streaming Responses | ‚úÖ OpenAI |
| Nested Chat | ‚úÖ Complete |
| Sequential Chat | ‚úÖ Complete |
| Context Management | ‚úÖ Complete |

### ‚è≥ Planned Features

For a comprehensive comparison and detailed roadmap of missing features, see [FEATURE_COMPARISON.md](./FEATURE_COMPARISON.md).

**High Priority Missing Features:**
- RAG/Retrieval Augmented Generation (RetrieveUserProxyAgent, Vector DB)
- Teachability System (Teachable agents, persistent learning)
- Observability (OpenTelemetry, tracing, logging)
- Docker Code Executor
- Azure OpenAI Support
- Model Context Protocol (MCP)
- Built-in Tools System
- Enterprise Features (Rate limiting, cost tracking)

## Roadmap

### ‚úÖ Completed Features
- [x] Base agent framework
- [x] AssistantAgent with OpenAI
- [x] UserProxyAgent
- [x] Group chat capabilities
- [x] Multiple LLM provider support (OpenAI, OpenRouter, Ollama, Anthropic, Gemini)
- [x] Function calling support
- [x] Code execution agent (JavaScript, Python, Bash)
- [x] Memory system (ListMemory implementation)
- [x] Event-driven architecture (AutoGen v0.4)
  - [x] AgentRuntime interface
  - [x] SingleThreadedAgentRuntime implementation
  - [x] AgentId and TopicId for addressing
  - [x] CancellationToken for async control
  - [x] Direct message passing (sendMessage)
  - [x] Publish/Subscribe messaging (publishMessage)
  - [x] State persistence and management
- [x] Streaming responses (OpenAI)
- [x] Nested chat support
- [x] Sequential chat patterns
- [x] Context management and compression

### üéØ High Priority (Next 2-4 months)
- [ ] RAG/Retrieval Augmented Generation
  - [ ] RetrieveUserProxyAgent
  - [ ] RetrieveAssistantAgent
  - [ ] Vector database integration (ChromaDB, Qdrant)
  - [ ] Embedding models
  - [ ] Document chunking and text splitting
- [ ] Teachability System
  - [ ] Teachable agents
  - [ ] TextAnalyzerAgent
  - [ ] Memo database
  - [ ] Persistent learning
- [ ] Observability & Telemetry
  - [ ] OpenTelemetry integration
  - [ ] Distributed tracing
  - [ ] Logging system
  - [ ] Cost tracking
- [ ] Docker Code Executor
  - [ ] Secure sandboxed execution
  - [ ] Resource limits
- [ ] Azure OpenAI Support
- [ ] Model Context Protocol (MCP)
- [ ] Built-in Tools System
- [ ] Enterprise features (Rate limiting, Quota management)

### üî∂ Medium Priority (4-6 months)
- [ ] Distributed runtime (multi-process/multi-machine)
- [ ] Advanced Group Chat patterns (RoundRobin, Selector)
- [ ] Additional LLM providers (Bedrock, Cohere, Hugging Face)
- [ ] Streaming for Anthropic and Gemini
- [ ] Vector memory backends
- [ ] Multimodal agent support
- [ ] Browser automation (Playwright)
- [ ] LangChain tool adapter
- [ ] Production deployment features

### üî∑ Lower Priority (6+ months)
- [ ] AutoGen Studio (GUI)
- [ ] Advanced agents (GraphRAG, Society of Mind)
- [ ] Additional executors (Jupyter, Azure Container)
- [ ] VS Code extension
- [ ] CLI tools

See [FEATURE_COMPARISON.md](./FEATURE_COMPARISON.md) for detailed feature comparison with microsoft/autogen.

## Contributing

Contributions are welcome! This project aims to maintain feature parity with the .NET version of AutoGen while adapting to Node.js/TypeScript best practices.

## License

MIT

## Acknowledgments

This project is inspired by and based on the architecture of [microsoft/autogen](https://github.com/microsoft/autogen). Special thanks to the AutoGen team for creating such a powerful framework.

## Related Projects

- [microsoft/autogen](https://github.com/microsoft/autogen) - Original Python implementation
- [microsoft/autogen (dotnet)](https://github.com/microsoft/autogen/tree/main/dotnet) - .NET implementation

