# autogen_node

A Node.js/TypeScript implementation of [microsoft/autogen](https://github.com/microsoft/autogen), providing a framework for building multi-agent AI systems with conversational agents.

## Overview

This project brings the powerful multi-agent orchestration capabilities of Microsoft's AutoGen framework to the Node.js ecosystem. It's designed based on the .NET code structure and class definitions, providing a familiar API for developers working with AutoGen in different languages.

## Features

- **Base Agent Framework**: Core interfaces and abstract classes for building custom agents
- **Multiple LLM Providers**: Support for OpenAI, OpenRouter, Ollama, Anthropic, and Google Gemini
  - **OpenAI**: GPT-3.5, GPT-4, and other OpenAI models
  - **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus, and Claude 3 Haiku
  - **Google Gemini**: Gemini 1.5 Flash, Gemini 1.5 Pro, and Gemini Pro
  - **OpenRouter**: Access to 100+ models from multiple providers
  - **Ollama**: Run LLMs locally for privacy and offline use
- **AssistantAgent**: LLM-powered conversational agent with provider flexibility
- **UserProxyAgent**: Human-in-the-loop agent for interactive conversations
- **Group Chat**: Multi-agent collaboration system for complex tasks
- **Function Calling**: Register and execute custom functions with agents
- **Code Execution**: Automatically execute code generated by agents (JavaScript, Python, Bash)
- **Type-Safe**: Built with TypeScript for enhanced developer experience
- **Flexible Message System**: Support for different message types and roles
- **Conversation Management**: Built-in conversation history and state management

## Installation

```bash
npm install
```

## Quick Start

### Using OpenAI (Default)

```typescript
import { AssistantAgent, UserProxyAgent, HumanInputMode } from './src/index';

// Create an AI assistant
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openai',  // optional, this is the default
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful assistant.',
  model: 'gpt-3.5-turbo',
  temperature: 0
});

// Create a user proxy for human interaction
const userProxy = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.ALWAYS
});

// Start a conversation
await userProxy.initiateChat(
  assistant,
  'Hello! Can you help me?',
  10 // max rounds
);
```

### Using OpenRouter

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openrouter',
  apiKey: process.env.OPENROUTER_API_KEY!,
  model: 'anthropic/claude-2',
  temperature: 0.7
});
```

### Using Anthropic Claude

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'anthropic',
  apiKey: process.env.ANTHROPIC_API_KEY!,
  model: 'claude-3-5-sonnet-20241022',
  temperature: 0.7
});
```

### Using Google Gemini

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'gemini',
  apiKey: process.env.GEMINI_API_KEY!,
  model: 'gemini-1.5-flash',
  temperature: 0.7
});
```

### Using Ollama (Local)

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'ollama',
  model: 'llama2',
  temperature: 0.7
});
```

See [LLM_PROVIDERS.md](./LLM_PROVIDERS.md) for detailed provider documentation.


## Project Structure

```
autogen_node/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                 # Core interfaces and base classes
â”‚   â”‚   â”œâ”€â”€ IAgent.ts         # Agent interface definitions
â”‚   â”‚   â”œâ”€â”€ BaseAgent.ts      # Base agent implementation
â”‚   â”‚   â”œâ”€â”€ IFunctionCall.ts  # Function calling interfaces
â”‚   â”‚   â”œâ”€â”€ FunctionContract.ts # Function contract builder
â”‚   â”‚   â”œâ”€â”€ FunctionCallMiddleware.ts # Function execution middleware
â”‚   â”‚   â””â”€â”€ ICodeExecutor.ts  # Code execution interface
â”‚   â”œâ”€â”€ agents/               # Agent implementations
â”‚   â”‚   â”œâ”€â”€ AssistantAgent.ts # LLM-powered assistant with function calling
â”‚   â”‚   â””â”€â”€ UserProxyAgent.ts # Human proxy with code execution
â”‚   â”œâ”€â”€ executors/            # Code execution implementations
â”‚   â”‚   â””â”€â”€ LocalCodeExecutor.ts # Local code executor
â”‚   â”œâ”€â”€ providers/            # LLM provider implementations
â”‚   â”‚   â”œâ”€â”€ OpenAIProvider.ts
â”‚   â”‚   â”œâ”€â”€ OpenRouterProvider.ts
â”‚   â”‚   â””â”€â”€ OllamaProvider.ts
â”‚   â”œâ”€â”€ examples/             # Example applications
â”‚   â”‚   â”œâ”€â”€ basic-chat.ts
â”‚   â”‚   â”œâ”€â”€ function-calling-example.ts
â”‚   â”‚   â””â”€â”€ code-execution-example.ts
â”‚   â””â”€â”€ index.ts              # Main export file
â”œâ”€â”€ dist/                     # Compiled JavaScript output
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## Architecture

This implementation follows the .NET AutoGen architecture:

### Core Components

1. **IAgent Interface**: Defines the contract for all agents
   - `generateReply()`: Generate responses to messages
   - `getName()`: Get the agent's name

2. **BaseAgent**: Abstract base class providing:
   - Conversation history management
   - Message sending and receiving
   - Chat initiation logic
   - Termination detection

3. **Agent Implementations**:
   - **AssistantAgent**: Uses LLM providers for intelligent responses with function calling support
   - **UserProxyAgent**: Facilitates human interaction with configurable input modes and code execution

4. **Function Calling**: Enable agents to call custom functions
   - Define functions with `FunctionContract`
   - Automatic function execution via `FunctionCallMiddleware`
   - OpenAI-compatible function definitions

5. **Code Execution**: Execute code generated by agents
   - `LocalCodeExecutor` for JavaScript, Python, and Bash
   - Automatic code extraction from markdown code blocks
   - Safe execution in temporary directories
### Message System

Messages follow a structured format:
```typescript
interface IMessage {
  content: string;
  role: 'user' | 'assistant' | 'system' | 'function' | 'tool';
  name?: string;
  functionCall?: {
    name: string;
    arguments: string;
  };
  toolCalls?: Array<{
    id: string;
    type: 'function';
    function: {
      name: string;
      arguments: string;
    };
  }>;
  toolCallId?: string;
}
```

## Configuration

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here  # Optional
GEMINI_API_KEY=your_gemini_api_key_here        # Optional
OPENROUTER_API_KEY=your_openrouter_api_key_here  # Optional
# OLLAMA_BASE_URL=http://localhost:11434/v1      # Optional
```

## Scripts

```bash
# Build the project
npm run build

# Run the basic interactive example (OpenAI)
npm run example:basic

# Run the automated two-agent conversation (OpenAI)
npm run example:auto

# Run the group chat example (OpenAI)
npm run example:group

# Run Anthropic Claude example
npm run example:anthropic

# Run Google Gemini example
npm run example:gemini

# Run OpenRouter example
npm run example:openrouter

# Run Ollama example (local LLM)
npm run example:ollama

# Run function calling example
npm run example:functions

# Run code execution example
npm run example:code

# Run tests
npm test

# Run tests with coverage
npm run test:coverage

# Development mode with auto-reload
npm run dev

# Clean build artifacts
npm run clean
```

## Examples

### Basic Two-Agent Chat

```typescript
import { AssistantAgent, UserProxyAgent, HumanInputMode } from './src/index';

const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful math tutor.',
  model: 'gpt-3.5-turbo'
});

const user = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.ALWAYS
});

await user.initiateChat(assistant, 'Help me solve 2x + 3 = 7', 10);
```

### Automated Conversation (No Human Input)

```typescript
const user = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.NEVER
});

// Agent will auto-reply without human intervention
```

### Function Calling

```typescript
import { AssistantAgent, FunctionContract } from './src/index';

// Define a weather function
const getWeather = FunctionContract.fromFunction(
  'get_weather',
  'Get the current weather for a location',
  [
    {
      name: 'location',
      type: 'string',
      description: 'The city and state, e.g. San Francisco, CA',
      required: true
    }
  ],
  async (location: string) => {
    // Your weather API logic here
    return `The weather in ${location} is sunny, 72Â°F`;
  }
);

// Create assistant with functions
const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful assistant with access to weather data.',
  model: 'gpt-3.5-turbo',
  functions: [getWeather]
});

// The assistant will automatically call the function when needed
await userProxy.initiateChat(assistant, "What's the weather in San Francisco?", 3);
```

### Code Execution

```typescript
import { AssistantAgent, UserProxyAgent, LocalCodeExecutor, HumanInputMode } from './src/index';

// Create code executor
const codeExecutor = new LocalCodeExecutor();

// Create assistant that writes code
const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a coding assistant. Write code in markdown code blocks.',
  model: 'gpt-3.5-turbo'
});

// Create user proxy with code execution enabled
const userProxy = new UserProxyAgent({
  name: 'user_proxy',
  humanInputMode: HumanInputMode.NEVER,
  codeExecutor: codeExecutor,
  autoExecuteCode: true
});

// The agent will write code, and it will be automatically executed
await userProxy.initiateChat(
  assistant,
  'Write JavaScript code to calculate the sum of numbers from 1 to 100',
  3
);

await codeExecutor.cleanup();
```

### Group Chat with Multiple Agents

```typescript
import { AssistantAgent, GroupChat, GroupChatManager } from './src/index';

// Create multiple specialized agents
const designer = new AssistantAgent({
  name: 'designer',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a creative designer.',
  model: 'gpt-3.5-turbo'
});

const engineer = new AssistantAgent({
  name: 'engineer',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a practical engineer.',
  model: 'gpt-3.5-turbo'
});

// Create group chat
const groupChat = new GroupChat({
  agents: [designer, engineer],
  maxRound: 10
});

// Create manager
const manager = new GroupChatManager({
  groupChat: groupChat
});

// Run the discussion
await manager.runChat('Design a new mobile app feature');
```

## Comparison with Microsoft AutoGen

### Quick Comparison

| Feature | microsoft/autogen | autogen_node |
|---------|-------------------|--------------|
| Base Agent Framework | âœ… | âœ… |
| AssistantAgent | âœ… | âœ… |
| UserProxyAgent | âœ… | âœ… |
| OpenAI Integration | âœ… | âœ… |
| Group Chat | âœ… | âœ… |
| Multiple LLM Providers | âœ… | âœ… (OpenAI, Anthropic, Gemini, OpenRouter, Ollama) |
| Function Calling | âœ… | âœ… |
| Code Execution | âœ… | âœ… (JavaScript, Python, Bash) |
| Event-Driven Architecture | âœ… | âŒ |
| RAG Support | âœ… | âŒ |
| Nested/Sequential Chats | âœ… | âŒ |
| AutoGen Studio | âœ… | âŒ |
| Streaming Responses | âœ… | âŒ |

### Detailed Feature Comparison

For a comprehensive comparison of features between `microsoft/autogen` and `autogen_node`, please see:

- **[åŠŸèƒ½å¯¹æ¯”æ–‡æ¡£ (Chinese)](./FEATURE_COMPARISON.md)** - å®Œæ•´çš„åŠŸèƒ½å¯¹æ¯”å’Œç¼ºå¤±åŠŸèƒ½åˆ—è¡¨
- **[Feature Comparison (English)](./FEATURE_COMPARISON_EN.md)** - Complete feature comparison and missing features
- **[åŠŸèƒ½å·®è·æ‘˜è¦ (Summary)](./FEATURE_GAP_SUMMARY.md)** - å¿«é€Ÿæ¦‚è§ˆå’Œä¼˜å…ˆçº§å»ºè®®

**Key Findings:**
- âœ… **Implemented**: ~9 core features
- âŒ **Missing**: ~78 features across 12 categories
  - ğŸ”´ High Priority: ~23 features (core capabilities)
  - ğŸŸ¡ Medium Priority: ~35 features (important enhancements)
  - ğŸŸ¢ Low Priority: ~20 features (nice to have)

## Roadmap

- [x] Base agent framework
- [x] AssistantAgent with OpenAI
- [x] UserProxyAgent
- [x] Group chat capabilities
- [x] Multiple LLM provider support (OpenAI, OpenRouter, Ollama)
- [x] Function calling support
- [x] Code execution agent (JavaScript, Python, Bash)
- [x] Additional LLM provider integrations (Anthropic SDK, Google Gemini)
- [ ] Advanced conversation patterns
- [ ] Streaming responses
- [ ] Performance optimizations

## Contributing

Contributions are welcome! This project aims to maintain feature parity with the .NET version of AutoGen while adapting to Node.js/TypeScript best practices.

## License

MIT

## Acknowledgments

This project is inspired by and based on the architecture of [microsoft/autogen](https://github.com/microsoft/autogen). Special thanks to the AutoGen team for creating such a powerful framework.

## Related Projects

- [microsoft/autogen](https://github.com/microsoft/autogen) - Original Python implementation
- [microsoft/autogen (dotnet)](https://github.com/microsoft/autogen/tree/main/dotnet) - .NET implementation

