# autogen_node

A Node.js/TypeScript implementation of [microsoft/autogen](https://github.com/microsoft/autogen), providing a framework for building multi-agent AI systems with conversational agents.

## Overview

This project brings the powerful multi-agent orchestration capabilities of Microsoft's AutoGen framework to the Node.js ecosystem. It's designed based on the .NET code structure and class definitions, providing a familiar API for developers working with AutoGen in different languages.

## Features

- **Event-Driven Architecture (AutoGen v0.4)**: Asynchronous message passing and distributed agent systems
  - **AgentRuntime**: Core runtime for hosting and managing agents
  - **Direct Messaging**: Send messages between agents asynchronously
  - **Publish/Subscribe**: Topic-based broadcast messaging
  - **Cancellation Tokens**: Control and cancel async operations
  - **State Management**: Persist and restore runtime state
- **Base Agent Framework**: Core interfaces and abstract classes for building custom agents
- **Multiple LLM Providers**: Support for OpenAI, OpenRouter, Ollama, Anthropic, and Google Gemini
  - **OpenAI**: GPT-3.5, GPT-4, and other OpenAI models
  - **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus, and Claude 3 Haiku
  - **Google Gemini**: Gemini 1.5 Flash, Gemini 1.5 Pro, and Gemini Pro
  - **OpenRouter**: Access to 100+ models from multiple providers
  - **Ollama**: Run LLMs locally for privacy and offline use
- **AssistantAgent**: LLM-powered conversational agent with provider flexibility
- **UserProxyAgent**: Human-in-the-loop agent for interactive conversations
- **Group Chat**: Multi-agent collaboration system for complex tasks
- **Function Calling**: Register and execute custom functions with agents
- **Code Execution**: Automatically execute code generated by agents (JavaScript, Python, Bash)
- **Memory System**: Persistent memory for agents to maintain context across conversations (based on Microsoft AutoGen)
- **Type-Safe**: Built with TypeScript for enhanced developer experience
- **Flexible Message System**: Support for different message types and roles
- **Conversation Management**: Built-in conversation history and state management
- **Advanced Conversation Patterns**: Complete implementation of AutoGen patterns
  - **Nested Chat**: Hierarchical conversations with task delegation
  - **Sequential Chat**: Predefined workflow execution
  - **Speaker Selection**: Multiple strategies (Round-robin, Random, Manual, Constrained, Auto/LLM-based)
  - **Swarm Mode**: Dynamic multi-agent task distribution and collaboration

## Installation

```bash
npm install
```

## Quick Start

### Using OpenAI (Default)

```typescript
import { AssistantAgent, UserProxyAgent, HumanInputMode } from './src/index';

// Create an AI assistant
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openai',  // optional, this is the default
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful assistant.',
  model: 'gpt-3.5-turbo',
  temperature: 0
});

// Create a user proxy for human interaction
const userProxy = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.ALWAYS
});

// Start a conversation
await userProxy.initiateChat(
  assistant,
  'Hello! Can you help me?',
  10 // max rounds
);
```

### Using OpenRouter

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openrouter',
  apiKey: process.env.OPENROUTER_API_KEY!,
  model: 'anthropic/claude-2',
  temperature: 0.7
});
```

### Using Anthropic Claude

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'anthropic',
  apiKey: process.env.ANTHROPIC_API_KEY!,
  model: 'claude-3-5-sonnet-20241022',
  temperature: 0.7
});
```

### Using Google Gemini

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'gemini',
  apiKey: process.env.GEMINI_API_KEY!,
  model: 'gemini-1.5-flash',
  temperature: 0.7
});
```

### Using Ollama (Local)

```typescript
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'ollama',
  model: 'llama2',
  temperature: 0.7
});
```

See [LLM_PROVIDERS.md](./LLM_PROVIDERS.md) for detailed provider documentation.


## Project Structure

```
autogen_node/
├── src/
│   ├── core/                 # Core interfaces and base classes
│   │   ├── IAgent.ts         # Agent interface definitions
│   │   ├── BaseAgent.ts      # Base agent implementation
│   │   ├── IFunctionCall.ts  # Function calling interfaces
│   │   ├── FunctionContract.ts # Function contract builder
│   │   ├── FunctionCallMiddleware.ts # Function execution middleware
│   │   └── ICodeExecutor.ts  # Code execution interface
│   ├── agents/               # Agent implementations
│   │   ├── AssistantAgent.ts # LLM-powered assistant with function calling
│   │   └── UserProxyAgent.ts # Human proxy with code execution
│   ├── executors/            # Code execution implementations
│   │   └── LocalCodeExecutor.ts # Local code executor
│   ├── providers/            # LLM provider implementations
│   │   ├── OpenAIProvider.ts
│   │   ├── OpenRouterProvider.ts
│   │   └── OllamaProvider.ts
│   ├── examples/             # Example applications
│   │   ├── basic-chat.ts
│   │   ├── function-calling-example.ts
│   │   └── code-execution-example.ts
│   └── index.ts              # Main export file
├── dist/                     # Compiled JavaScript output
├── package.json
├── tsconfig.json
└── README.md
```

## Architecture

This implementation follows the AutoGen architecture with both traditional and event-driven patterns:

### Event-Driven Architecture (AutoGen v0.4)

The new event-driven architecture enables scalable, distributed multi-agent systems:

1. **AgentRuntime**: Core runtime for hosting and managing agents
   - `sendMessage()`: Direct asynchronous message passing
   - `publishMessage()`: Topic-based broadcast messaging
   - Agent registration and lifecycle management
   - State persistence and restoration

2. **AgentId & TopicId**: Distributed agent addressing
   - Unique identification for agents across processes
   - Topic-based message routing

3. **CancellationToken**: Async operation control
   - Cancel long-running operations
   - Cleanup on cancellation

See [EVENT_DRIVEN.md](./EVENT_DRIVEN.md) for detailed documentation.

### Traditional Architecture

1. **IAgent Interface**: Defines the contract for all agents
   - `generateReply()`: Generate responses to messages
   - `getName()`: Get the agent's name

2. **BaseAgent**: Abstract base class providing:
   - Conversation history management
   - Message sending and receiving
   - Chat initiation logic
   - Termination detection

3. **Agent Implementations**:
   - **AssistantAgent**: Uses LLM providers for intelligent responses with function calling support
   - **UserProxyAgent**: Facilitates human interaction with configurable input modes and code execution

4. **Function Calling**: Enable agents to call custom functions
   - Define functions with `FunctionContract`
   - Automatic function execution via `FunctionCallMiddleware`
   - OpenAI-compatible function definitions

5. **Code Execution**: Execute code generated by agents
   - `LocalCodeExecutor` for JavaScript, Python, and Bash
   - Automatic code extraction from markdown code blocks
   - Safe execution in temporary directories
### Message System

Messages follow a structured format:
```typescript
interface IMessage {
  content: string;
  role: 'user' | 'assistant' | 'system' | 'function' | 'tool';
  name?: string;
  functionCall?: {
    name: string;
    arguments: string;
  };
  toolCalls?: Array<{
    id: string;
    type: 'function';
    function: {
      name: string;
      arguments: string;
    };
  }>;
  toolCallId?: string;
}
```

## Configuration

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here  # Optional
GEMINI_API_KEY=your_gemini_api_key_here        # Optional
OPENROUTER_API_KEY=your_openrouter_api_key_here  # Optional
# OLLAMA_BASE_URL=http://localhost:11434/v1      # Optional
```

## Scripts

```bash
# Build the project
npm run build

# Run the basic interactive example (OpenAI)
npm run example:basic

# Run the automated two-agent conversation (OpenAI)
npm run example:auto

# Run the group chat example (OpenAI)
npm run example:group

# Run Anthropic Claude example
npm run example:anthropic

# Run Google Gemini example
npm run example:gemini

# Run OpenRouter example
npm run example:openrouter

# Run Ollama example (local LLM)
npm run example:ollama

# Run function calling example
npm run example:functions

# Run code execution example
npm run example:code

# Run memory example
npm run example:memory

# Run nested chat example
npm run example:nested

# Run sequential chat example
npm run example:sequential

# Run speaker selection strategies example
npm run example:speaker

# Run swarm mode example
npm run example:swarm

# Run event-driven architecture example (AutoGen v0.4)
npm run example:events

# Run tests
npm test

# Run tests with coverage
npm run test:coverage

# Development mode with auto-reload
npm run dev

# Clean build artifacts
npm run clean
```

## Examples

### Event-Driven Architecture (AutoGen v0.4)

```typescript
import {
  AgentId,
  TopicId,
  SingleThreadedAgentRuntime,
  createSubscription,
} from './src/index';

// Create event-driven agent
class EventAgent {
  async handleMessage(message: any, sender: AgentId | null) {
    return {
      role: 'assistant',
      content: `Processed: ${message.content}`,
    };
  }
}

// Create runtime and register agents
const runtime = new SingleThreadedAgentRuntime();
const agent = new EventAgent();
const agentId = new AgentId('event_agent', 'agent1');

await runtime.registerAgentInstance(agent, agentId);

// Direct message passing
const response = await runtime.sendMessage(
  { content: 'Hello!' },
  agentId
);

// Topic-based pub/sub
const topic = new TopicId('notifications', 'system');
await runtime.addSubscription(
  createSubscription('sub1', topic, agentId)
);
await runtime.publishMessage(
  { content: 'Broadcast message' },
  topic
);
```

See [EVENT_DRIVEN.md](./EVENT_DRIVEN.md) for complete documentation.

### Basic Two-Agent Chat

```typescript
import { AssistantAgent, UserProxyAgent, HumanInputMode } from './src/index';

const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful math tutor.',
  model: 'gpt-3.5-turbo'
});

const user = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.ALWAYS
});

await user.initiateChat(assistant, 'Help me solve 2x + 3 = 7', 10);
```

### Automated Conversation (No Human Input)

```typescript
const user = new UserProxyAgent({
  name: 'user',
  humanInputMode: HumanInputMode.NEVER
});

// Agent will auto-reply without human intervention
```

### Function Calling

```typescript
import { AssistantAgent, FunctionContract } from './src/index';

// Define a weather function
const getWeather = FunctionContract.fromFunction(
  'get_weather',
  'Get the current weather for a location',
  [
    {
      name: 'location',
      type: 'string',
      description: 'The city and state, e.g. San Francisco, CA',
      required: true
    }
  ],
  async (location: string) => {
    // Your weather API logic here
    return `The weather in ${location} is sunny, 72°F`;
  }
);

// Create assistant with functions
const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a helpful assistant with access to weather data.',
  model: 'gpt-3.5-turbo',
  functions: [getWeather]
});

// The assistant will automatically call the function when needed
await userProxy.initiateChat(assistant, "What's the weather in San Francisco?", 3);
```

### Code Execution

```typescript
import { AssistantAgent, UserProxyAgent, LocalCodeExecutor, HumanInputMode } from './src/index';

// Create code executor
const codeExecutor = new LocalCodeExecutor();

// Create assistant that writes code
const assistant = new AssistantAgent({
  name: 'assistant',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a coding assistant. Write code in markdown code blocks.',
  model: 'gpt-3.5-turbo'
});

// Create user proxy with code execution enabled
const userProxy = new UserProxyAgent({
  name: 'user_proxy',
  humanInputMode: HumanInputMode.NEVER,
  codeExecutor: codeExecutor,
  autoExecuteCode: true
});

// The agent will write code, and it will be automatically executed
await userProxy.initiateChat(
  assistant,
  'Write JavaScript code to calculate the sum of numbers from 1 to 100',
  3
);

await codeExecutor.cleanup();
```

### Group Chat with Multiple Agents

```typescript
import { AssistantAgent, GroupChat, GroupChatManager } from './src/index';

// Create multiple specialized agents
const designer = new AssistantAgent({
  name: 'designer',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a creative designer.',
  model: 'gpt-3.5-turbo'
});

const engineer = new AssistantAgent({
  name: 'engineer',
  apiKey: process.env.OPENAI_API_KEY!,
  systemMessage: 'You are a practical engineer.',
  model: 'gpt-3.5-turbo'
});

// Create group chat
const groupChat = new GroupChat({
  agents: [designer, engineer],
  maxRound: 10
});

// Create manager
const manager = new GroupChatManager({
  groupChat: groupChat
});

// Run the discussion
await manager.runChat('Design a new mobile app feature');
```

### Advanced Conversation Patterns

autogen_node implements all major conversation patterns from Microsoft AutoGen:

#### Nested Chat

Delegate tasks to specialist agents:

```typescript
import { AssistantAgent, supportsNestedChat } from './src/index';

const projectManager = new AssistantAgent({
  name: 'project_manager',
  systemMessage: 'You delegate tasks to specialists.',
  apiKey: process.env.OPENAI_API_KEY!
});

const specialist = new AssistantAgent({
  name: 'specialist',
  systemMessage: 'You are a code review specialist.',
  apiKey: process.env.OPENAI_API_KEY!
});

// Delegate task to specialist
const result = await projectManager.initiateNestedChat(
  'Review this code: ...',
  specialist,
  { maxRounds: 3, addToParentHistory: true }
);
```

#### Sequential Chat

Execute agents in predefined workflow order:

```typescript
import { runSequentialChat, AssistantAgent } from './src/index';

const result = await runSequentialChat({
  steps: [
    { agent: researcher, maxRounds: 1 },
    { agent: writer, maxRounds: 1 },
    { agent: editor, maxRounds: 1 }
  ],
  initialMessage: 'Write an article about AI'
});
```

#### Speaker Selection Strategies

Control who speaks next in group chats:

```typescript
import { GroupChat, RoundRobinSelector, RandomSelector, AutoSelector } from './src/index';

// Round-robin selection
const chat1 = new GroupChat({
  agents: [agent1, agent2, agent3],
  speakerSelector: new RoundRobinSelector()
});

// Random selection
const chat2 = new GroupChat({
  agents: [agent1, agent2, agent3],
  speakerSelector: new RandomSelector()
});

// LLM-based intelligent selection
const coordinator = new AssistantAgent({ ... });
const chat3 = new GroupChat({
  agents: [agent1, agent2, agent3],
  speakerSelector: new AutoSelector({ selectorAgent: coordinator })
});
```

#### Swarm Mode

Distribute tasks among agents dynamically:

```typescript
import { SwarmChat, RoundRobinSelector } from './src/index';

const swarm = new SwarmChat({
  agents: [researcher, writer, coder, reviewer],
  maxRoundsPerTask: 3,
  taskAssignmentSelector: new RoundRobinSelector()
});

const result = await swarm.run([
  'Research TypeScript benefits',
  'Write a tutorial',
  'Create code examples',
  'Review documentation'
]);

console.log(`Completed: ${result.completedTasks.length}`);
```

See [CONVERSATION_PATTERNS.md](./CONVERSATION_PATTERNS.md) for detailed documentation.

### Memory Usage

Memory allows agents to maintain context across conversations:

```typescript
import { AssistantAgent, ListMemory, MemoryMimeType } from './src/index';

// Create memory instance
const userMemory = new ListMemory({ name: 'user_preferences' });

// Add memory content
await userMemory.add({
  content: 'User prefers formal language',
  mimeType: MemoryMimeType.TEXT,
  metadata: { timestamp: Date.now() }
});

await userMemory.add({
  content: 'User is interested in TypeScript and AI',
  mimeType: MemoryMimeType.TEXT
});

// Create agent with memory
const assistant = new AssistantAgent({
  name: 'assistant',
  provider: 'openai',
  apiKey: process.env.OPENAI_API_KEY!,
  memory: [userMemory]
});

// Memory is automatically injected into context
const reply = await assistant.generateReply([
  { role: 'user', content: 'What should I learn next?' }
]);
```

For more details, see [MEMORY.md](MEMORY.md).

## Comparison with .NET AutoGen

| Feature | .NET AutoGen | autogen_node |
|---------|--------------|--------------|
| Base Agent Framework | ✅ | ✅ |
| AssistantAgent | ✅ | ✅ |
| UserProxyAgent | ✅ | ✅ |
| ConversableAgent | ✅ | ✅ |
| RetrieveUserProxyAgent (RAG) | ✅ | ✅ |
| GPTAssistantAgent | ✅ | ✅ |
| MultimodalConversableAgent | ✅ | ✅ |
| TeachableAgent | ✅ | ✅ |
| CompressibleAgent | ✅ | ✅ |
| SocietyOfMindAgent | ✅ | ✅ |
| OpenAI Integration | ✅ | ✅ |
| Group Chat | ✅ | ✅ |
| Multiple LLM Providers | ✅ | ✅ (OpenAI, Anthropic, Gemini, OpenRouter, Ollama) |
| Function Calling | ✅ | ✅ |
| Code Execution | ✅ | ✅ (JavaScript, Python, Bash) |
| Memory System | ✅ | ✅ (Based on Python AutoGen) |
| Event-Driven Architecture (v0.4) | ✅ | ✅ |
| AgentRuntime | ✅ | ✅ (SingleThreadedAgentRuntime) |
| Async Message Passing | ✅ | ✅ |
| Publish/Subscribe | ✅ | ✅ |

## Advanced Agent Types

autogen_node now includes all major agent types from Microsoft AutoGen:

- **ConversableAgent**: Flexible agent with optional LLM integration and configurable behaviors
- **RetrieveUserProxyAgent**: RAG-enabled agent for document Q&A and knowledge base queries
- **GPTAssistantAgent**: Integration with OpenAI's Assistant API for persistent conversations
- **MultimodalConversableAgent**: Support for images, audio, and multimodal interactions
- **TeachableAgent**: Learns user preferences and provides personalized responses
- **CompressibleAgent**: Manages long conversations with automatic history compression
- **SocietyOfMindAgent**: Complex reasoning using multiple specialized inner agents

For detailed documentation and examples, see [ADVANCED_AGENTS.md](./ADVANCED_AGENTS.md).

## Roadmap

- [x] Base agent framework
- [x] AssistantAgent with OpenAI
- [x] UserProxyAgent
- [x] Group chat capabilities
- [x] Multiple LLM provider support (OpenAI, OpenRouter, Ollama)
- [x] Function calling support
- [x] Code execution agent (JavaScript, Python, Bash)
- [x] Additional LLM provider integrations (Anthropic SDK, Google Gemini)
- [x] Memory system (ListMemory implementation)
- [x] Event-driven architecture (AutoGen v0.4)
  - [x] AgentRuntime interface
  - [x] SingleThreadedAgentRuntime implementation
  - [x] AgentId and TopicId for addressing
  - [x] CancellationToken for async control
  - [x] Direct message passing (sendMessage)
  - [x] Publish/Subscribe messaging (publishMessage)
  - [x] State persistence and management
  - [ ] Distributed runtime (multi-process/multi-machine)
- [x] Advanced agent types
  - [x] ConversableAgent (flexible conversable agent)
  - [x] RetrieveUserProxyAgent (RAG support)
  - [x] GPTAssistantAgent (OpenAI Assistant API)
  - [x] MultimodalConversableAgent (image/audio support)
  - [x] TeachableAgent (learning and personalization)
  - [x] CompressibleAgent (conversation compression)
  - [x] SocietyOfMindAgent (multi-agent reasoning)
- [x] Advanced conversation patterns
  - [x] Nested Chat (task delegation)
  - [x] Sequential Chat (workflow automation)
  - [x] Speaker Selection Strategies (Round-robin, Random, Manual, Constrained, Auto)
  - [x] Swarm Mode (dynamic multi-agent collaboration)
- [ ] Streaming responses
- [ ] Performance optimizations
- [ ] Additional memory backends (Vector, Database, File-based)

## Contributing

Contributions are welcome! This project aims to maintain feature parity with the .NET version of AutoGen while adapting to Node.js/TypeScript best practices.

## License

MIT

## Acknowledgments

This project is inspired by and based on the architecture of [microsoft/autogen](https://github.com/microsoft/autogen). Special thanks to the AutoGen team for creating such a powerful framework.

## Related Projects

- [microsoft/autogen](https://github.com/microsoft/autogen) - Original Python implementation
- [microsoft/autogen (dotnet)](https://github.com/microsoft/autogen/tree/main/dotnet) - .NET implementation

